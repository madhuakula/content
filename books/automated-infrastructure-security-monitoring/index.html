<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

  <title>Automated Infrastructure Security Monitoring & Defence - null Bangalore</title>
  <link rel="stylesheet" href="./css/reveal.css" />
  <link rel="stylesheet" href="./css/theme/black.css" id="theme" />
  <link rel="stylesheet" href="./css/highlight/zenburn.css" />
  <link rel="stylesheet" href="./css/print/paper.css" type="text/css" media="print" />

</head>

<body>
  <div class="reveal">
    <div class="slides">
      <section data-markdown>
        <script type="text/template">## Automated Infrastructure Security Monitoring & Defence

#### **Madhu Akula**

null Bangalore Bachaav, 10th December 2016

</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## Hello everyone :)
#### Welcome to awesome workshop!

All files, references and resources can be found at [https://github.com/madhuakula/nullblr-bachaav-aismd](https://github.com/madhuakula/nullblr-bachaav-aismd)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## SCHEDULE

**Saturday, 10th December 2016**<br />
**09:30 – 18:00**
<br /><br />

There will be
- 20 minute break at 11:30
- 1 hour break at 13:00
- 20 minute break at 16:00
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Pre-requisites
- This workshop is intended for beginner to mid-level, we are expecting that participants are comfortable with basic Linux CLI usage
- Laptop with administrative privileges (to install VirtualBox)
- VirtualBox 5 (or) above
- Python 2.7.x
- 10GB hard disk space for virtual machines
- Minimum 4 GB RAM
- Enthusiasm to learn cool stuff :)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## Instructions
1. Please follow the commands and the overall flow as given in the slides
2. Please direct all questions/queries to instructors (or) volunteers
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## What will you learn today?
- Infrastructure monitoring by aggregating and analysing logs
- Centralised logging using the ELK stack in near real-time
- Creating attack pattern dashboards for Alerting & Monitoring
- Exporting and Importing dashboards for reporting and reuse
- Advanced configurations of the ELK stack
- Automated Defence with collected data
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## (Bonus)
**Best practices and Security Tips**
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
## What we are not covering
- Performance tuning and optimisation for clusters
- Multi cluster configurations
- Custom plugins and scripts for Logstash

<aside class="notes"><p>Time constraint + Internet access + Resource constraints</p>
</aside></script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### So what is Automated Infrastructure Security Monitoring & Defence?
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### What is monitoring?
![](images/monitor.png)

<aside class="notes"><p>observe an event and check the progress over a period of time.</p>
</aside></script>
        </section>
        <section data-markdown>
          <script type="text/template">
### What is this 'infrastructure' we keep talking about?
>![](images/itinfra.png)

<aside class="notes"><p>All your assets in an enterprisey environment. The routers, firewalls, web app servers, linux boxes, ldap servers, database servers etc.</p>
</aside></script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## About Me
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Madhu Akula

- Automation Ninja @Appsecco
- Security, Cloud & DevOps lover
- Trainer & Speaker [Defcon, DevSecCon, DevOps Days IN, All Day DevOps, etc.]
- Acknowledged by more than 200 giant companies like Google, Microsoft, Yahoo, Adobe, etc for finding security vulnerabilities
- Open source contributor
- Never ending learner!
- Twitter: [@madhuakula](https://twitter.com/@madhuakula)
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Quick look at the basics of "centralised" logging
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## What are logs?

- A log (file) is a collection of records or events
- Logs used to be a (often indecipherable) line of text intended for offline human analysis of what went wrong
- Logs are a critical part of any system giving you an insight into the working of a system
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## Why is it boring to work with logs?
- Can be pretty large
- Correlation can be painful
- Others?
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## What is centralised logging?

- Managing logs and accessing them can get complicated with multiple hosts
- Searching for a particular error across hundreds of log files on hundreds of servers is difficult without good tools
- A common approach to this problem is to setup a centralised logging solution so that multiple logs can be aggregated in a central location

</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### How is it different from traditional logging?
- Logs are collected at a central server
- Parsing becomes simpler since data is accessible at a single location
- A common issue across multiple hosts/services can be identified by correlating specific time frames
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### What are the problems of traditional logging?
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
#### No Consistency
(it’s difficult to be jack-of-all trades)

- Difficulty in logging for each application, system, device
- Interpreting various type of logs
- Variation in format makes it challenging to search
- Many types of time formats
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
#### No Centralisation
(simply put, log data is everywhere)

- Logs in many locations on various servers
- SSH + GREP don’t scale
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
#### Accessibility of Log Data
(much of the data is difficult to locate and manage)

- Access is often difficult
- High expertise to mine data
- Logs can be difficult to find
- Immense size of Log Data
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## The ELK stack
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Elasticsearch, Logstash and Kibana
Different open source modules working together
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- Helps users/admins to collect, analyse and visualise data in (near) real-time
- Each module fits based on your use case and environment
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Components of the stack

- Elasticsearch
- Logstash
- Kibana
- (Beats)
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
##![](images/elasticsearch_def.png)
<small>Ref: https://www.elastic.co/products</small>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## Elasticsearch
- Distributed and Highly available search engine, written in Java and uses Groovy
- Built on top of Lucene
- Multi Tenant with Multi types and a set of APIs
- Document Oriented providing (near) real time search
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## ![](images/logstash_def.png)
<small>Ref: https://www.elastic.co/products</small>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## Logstash
- Tool for managing events and logs written in Ruby
- Centralised data processing of all types of logs
- Consists of 3 main components
    + Input : Passing logs to process them into machine understandable format
    + Filter : Set of conditions to perform specific action on a event
    + Output : Decision maker for processed events/logs
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## ![](images/kibana_def.png)
<small>Ref: https://www.elastic.co/products</small>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Kibana
- Powerful front-end dashboard written in JavaScript
- Browser based analytics and search dashboard for Elasticsearch
- Flexible analytics & visualisation platform
- Provides data in the form of charts, graphs, counts, maps, etc. in real-time
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## ![](images/beats_def.png)
<small>Ref: https://www.elastic.co/products</small>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Beats
- Lightweight shippers for Elasticsearch & Logstash
- Capture all sorts of operational data like logs or network packet data
- It can send logs to either elasticsearch, logstash
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- Different kind of beats
    + Libbeat : The Go framework for creating new Beats
    + Packetbeat : Tap into your wire data
    + Filebeat : Lightweight log forwarder to Logstash & Elasticsearch
    + Winlogbeat : Sends windows event logs
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
#### Filebeat
- Lightweight Shipper for Log Data
- Filebeat is an opensource file harvester
- Used to fetch logs files and feed them into logstash
- It has replaced logstash-forwarder
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
## ELK overview
>![](images/elk_overall.png)
<small>Ref: https://www.elastic.co/products</small>
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## A quick history lesson
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### About Elastic

- `Shay Banon` created Compass in 2004, third version of Compass is the first version of Elasticsearch, released in Feb 2010
- Elasticsearch was founded in 2012, Rashid (Kibana) joined Jan 2013 and Jordan (Logstash) joined in Aug 2013. (elastic.co)
- Combined stack is now called `Elastic Stack` and `X-Pack`
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## Terms we should be aware of in context to ELK
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Node

> A node is a running instance of elasticsearch which belongs to a cluster.
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Cluster

> Cluster is a collection of one or more nodes that together holds your entire data and provides indexing and search capabilities across all nodes.

<br />
> Nodes must have the same cluster.name to belong to the same cluster.
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Document

> A document is a basic unit of information that can be stored and searched.

<br />

>It refers to the top-level, or root object that is serialised into JSON and stored in Elasticsearch under a unique ID.
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Index

> An index is a collection of documents that have somewhat similar characteristics. It has mappings which define multiple types.
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Type

> Within an index, you can define one or more types. A type is a logical category/partition of your index whose semantics are completely up to you.
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Shard

> Elasticsearch provides the ability to subdivide an index into multiple pieces called shards since an index can store a large amount of data that can exceed the hardware limits of a single node.

<br />
> Each shard is in itself a fully-functional and independent "index" that can be hosted on any node in the cluster.

<aside class="notes"><p>For eg, a single index of a billion docs taking up 1TB of disk space may not fit on the disk of a single node or may be too slow to serve search requests from a single node alone.</p>
<p>When you create an index, you can simply define the number of shards that you want.</p>
</aside></script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Replica

>Elasticsearch allows you to make one or more copies of your index’s shards into what are called replica shards, or replicas for short.

<aside class="notes"><p>For this reason, it is important to note that a replica shard is never allocated on the same node as the original/primary shard that it was copied from
It provides high availability in case a shard/node fails. Also to scale out your search volume/throughput since searches can be executed on all replicas in parallel</p>
</aside></script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Snapshot and Restore

> A copy of the current state and data in a cluster that is saved to a shared repository.

> Elasticsearch provides a snapshot API to achieve this.

</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Routing

> A mechanism in Elasticsearch that identifies which shard a particular document resides in.
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Near real time

> Elasticsearch is a near real time search platform.

<br />

> What this means is there is a slight latency (normally one second) from the time you index a document until the time it becomes searchable.
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Workshop architecture

![Architecture](images/ourstructure.png)
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
# Hands-On
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Import virtual machine appliance

Please import the virtual machine appliance from

`null-bachaav-aismd/Virtual-Machines/NLIMW-Vanilla-Appliance.ova`

![import virtualbox](images/setup/vbox_import_ova.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
![Select OVA file location](images/setup/vbox-import-dailog.bmp)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
![import virtualbox](images/setup/import.png)
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Start the both virtual machines
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Login to the both machines

**username :: password**

infra::infra
monitor::monitor
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
# Please note

- In this entire documentation. Please change the IP address as you have got
    + `192.168.56.101` is the `monitor` VM
    + `192.168.56.100` is the `infra` VM

</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### SSH into the monitor VM

switch to `root` user by entering the below command

```
ssh -l monitor 192.168.56.101
sudo -i
```
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Configuration Paths in Monitor VM

**Logstash**
```
configuration = /etc/logstash/conf.d/
installation = /opt/logstash/bin/logstash
logs = /var/log/logstash
```

**elasticsearch**
```
configuration file = /etc/elasticsearch/elasticsearch.yml
installation = /usr/share/elasticsearch/bin/elasticsearch
logs = /var/log/elasticsearch
backups = /var/backups/elasticsearch
```

**Kibana**
```
configuration file = /opt/kibana/config/kibana.yml
installation = /opt/kibana/bin/kibana
logs = /var/log/kibana
```
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Configure Elasticsearch Cluster

- Replace the below content using your choice of text editor. `vi /etc/elasticsearch/elasticsearch.yml`

```
cluster.name: ninja-infra-mon

node.name: node-1

bootstrap.memory_lock: true

network.host: 127.0.0.1

path.repo: "/var/backups/elasticsearch"

```

<aside class="notes"><p>make sure check the spaces before editing in the file</p>
</aside></script>
        </section>
        <section data-markdown>
          <script type="text/template">
- Start Elasticsearch service by running

```
service elasticsearch restart
```
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
- Check whether Elasticsearch is running or not

```
curl -XGET 'http://localhost:9200'
```

![Elasticsearch CURL](images/elastic-search-curl.bmp)

<aside class="notes"><p>It will take 3 to 5 seconds to start elasticsearch</p>
</aside></script>
        </section>
        <section data-markdown>
          <script type="text/template">
- Check the Elasticsearch cluster status

```
curl -XGET 'http://localhost:9200/_cluster/health?pretty'

curl -XGET 'http://localhost:9200/_cluster/state?pretty'
```

- <font color="green">`green`</font> - All primary and replica shards are active.
- <font color="yellow">`yellow`</font> - All primary shards are active, but not all replica shards are active.
- <font color="red">`red`</font> - Not all primary shards are active.
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## So What happens in a multi node cluster?
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Clustering

> Elasticsearch is built to be always available, and to scale. Scale can come from buying bigger servers (vertical scale, or scaling up) or from buying more servers (horizontal scale, or scaling out).

<aside class="notes"><p>We need to start by understanding the word &#39;Clustering&#39; in context with Elasticsearch</p>
</aside></script>
        </section>
        <section data-markdown>
          <script type="text/template">
### An Empty Cluster

Our cluster with a single node, with no data and no indices:

![A cluster with one empty node](images/es-c-1.png)

- A node is a running instance of Elasticsearch
- A Cluster consists of one or more nodes with the same cluster.name

<aside class="notes"><p>By default Elasticsearch will have 5 primary shards and 1 replica shard with cluster name <code>elasticsearch</code> and node name as a random Marvel character.</p>
</aside></script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Adding index

- In an empty one-node cluster, we will assign three primary shards and one replica
- There is one replica for all the primary shards

![A single-node cluster with an index](images/es-c-2.png)

<aside class="notes"><p>Because there is no separate node, the replica shard becomes unassigned. Which is why it is not denoted in this image.</p>
</aside></script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Add Failover

- Running a single node means that you have a single point of failure - there is no redundancy
- We can simply start another node to prevent data loss

![A two-node cluster—all primary and replica shards are allocated](images/es-c-3.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Scale Horizontally

- One shard each from Node 1 and Node 2 have moved to the new Node 3

![A three-node cluster—shards have been reallocated to spread the load](images/es-c-4.png)
<aside class="notes"><p>This means that the hardware resources (CPU, RAM, I/O) of each node are being shared among fewer shards, allowing each shard to perform better</p>
</aside></script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Scale more

Three primaries and six replicas. This means that we can scale out to a total of nine nodes, again with one shard per node

![Increasing the number_of_replicas to 2](images/es-c-5.png)

<aside class="notes"><p>This would allow us to triple search performance compared to our original three-node cluster</p>
</aside></script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Failover Test

- A cluster must have a master node in order to function correctly
- When a master node dies, the nodes elect a new master

![Cluster after killing one node](images/es-c-6.png)
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Restful APIs over `HTTP` (CURL)

```
curl -X<VERB> '<SCHEMA>://<HOST>/<PATH>?<QUERY_STRING>' -d '<BODY>'
```

<aside class="notes"><p><code>VERB</code> - The appropriate HTTP method or verb: GET, POST, PUT, HEAD, or DELETE.<br />
<code>SCHEMA</code> - Either http or https (if you have an https proxy in front of Elasticsearch.)<br />
<code>HOST</code> - The hostname of any node in your Elasticsearch cluster, or localhost for a node on your local machine.<br />
<code>PORT</code> - The port running the Elasticsearch HTTP service, which defaults to 9200.<br />
<code>QUERY_STRING</code> - Any optional query-string parameters (for example ?pretty will pretty-print the JSON response to make it easier to read.)<br />
<code>BODY</code> - A JSON encoded request body (if the request needs one.)<br /></p>
</aside></script>
      </section>
      <section data-markdown>
        <script type="text/template">
### What is CRUD?

C - Create
R - Retrieve
U - Update
D - Delete
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### CRUD operations over Elasticsearch
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
<div align="left">Simple Index Creation with XPUT:</div>

```
curl -XPUT 'http://localhost:9200/null/'
```

<div align="left">Add data to your created index:</div>

```
curl -XPUT 'http://localhost:9200/null/workshop/1' -d '{"user":"ninja"}'
```

<div align="left">To check the Index status:</div>

```
curl -XGET 'http://localhost:9200/null/?pretty'
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
<div align="left">Automatic doc creation in an index with XPOST:</div>

```
curl -XPOST 'http://localhost:9200/null/workshop/' -d '{"user":"ninja"}'
```

<div align="left">Creating a user profile doc:</div>

```
curl -XPUT 'http://localhost:9200/null/workshop/9' -d '{"user":"admin", "role":"tester", "job":"engineer"}'
```

<div align="left">Update the document:</div>

```
curl -XPOST 'http://localhost:9200/null/workshop/9' -d '{"user":"administrator", "role":"tester", "job":"engineer"}'
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
<div align="left">Searching a doc in an index. Create 2 docs:</div>

```
curl -XPOST 'http://localhost:9200/null/workshop/' -d '{"user":"abcd", "role":"tester", "job":"engineer"}'
```

```
curl -XPOST 'http://localhost:9200/null/workshop/' -d '{"user":"abcd", "role":"admin", "job":"engineer"}'
```

<div align="left">Then search:</div>

```
curl -XGET 'http://localhost:9200/null/_search?q=user:abcd&pretty'
```


<div align="left">Deleting a doc in an index:</div>

```
curl -XDELETE 'http://localhost:9200/null/workshop/1'
```
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Setting up nginx reverse-proxy for elasticsearch and kibana

- Generate the basic authentication password by running `htpasswd`

```
htpasswd -c /etc/nginx/htpasswd.users elkadmin
```

```
Password : Null@123
Confirm Password : Null@123
```

![htpasswd generation](images/htpasswd-gen.bmp)
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
- Replace the nginx default configuration by editing  `vi /etc/nginx/sites-available/default`

```
server {
    listen 80; #for Kibana

    server_name localhost;

    auth_basic "Restricted Access";
    auth_basic_user_file /etc/nginx/htpasswd.users;

    location / {
        proxy_pass http://localhost:5601;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}

server {
    listen 8080; #for Elasticsearch

    server_name localhost;

    auth_basic "Restricted Access";
    auth_basic_user_file /etc/nginx/htpasswd.users;

    location / {
        proxy_pass http://localhost:9200;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}
```

- Restart the `nginx` service to apply the changes

```
service nginx restart
```
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Elasticsearch plugins overview

Plugins already installed in the system
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
Navigate to http://192.168.56.101:8080/_plugin/head

- Use the username `elkadmin` and password `Null@123`

![ES basic auth](images/es-basic-auth.bmp)
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
![ES head plugin](images/elasticsearch-head-plugin.png)

<aside class="notes"><p>Please use your monitor VM IP</p>
</aside></script>
      </section>
      <section data-markdown>
        <script type="text/template">
![ES Head Plugin](images/es-head-plugin-1.png)
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
![ES Head plugin](images/es-head-plugin2.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
Navigate to http://192.168.56.100:8080/_plugin/hq

![](images/elasticsearch-hq-plugin.png)

<aside class="notes"><p>Please use your monitor VM IP</p>
</aside></script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Let's configure Logstash

![Logstash Overview](images/logstash.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Logstash components

Logstash consists of 3 main components

- Input: Passing logs to process them into machine understandable format
- Filters: Set of conditionals to perform specific action on a event
- Output: Decision maker for processed events/logs
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Basic Logstash configuration

```
input {
    stdin {}
    file {}
    ...
}

filter {
    grok {}
    date {}
    geoip {}
    ...
}

output {
   elasticsearch {}
   email {}
   ...
}
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
#### Input

> An input plugin enables a specific source of events to be read by Logstash.

- File
- Lumberjack
- S3
- Beats
- Stdin
- Many more.
[https://www.elastic.co/guide/en/logstash/current/input-plugins.html](https://www.elastic.co/guide/en/logstash/current/input-plugins.html)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
#### Filter

> A filter plugin performs intermediary processing on an event. Filters are often applied conditionally depending on the characteristics of the event.

- CSV
- GeoIP
- Mutate
- Grok
- Many More.
[https://www.elastic.co/guide/en/logstash/current/filter-plugins.html](https://www.elastic.co/guide/en/logstash/current/filter-plugins.html)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
#### Output

> An output plugin sends event data to a particular destination. Outputs are the final stage in the event pipeline.

- Elasticsearch
- Email
- Stdout
- S3, file
- HTTP
- Many More.
[https://www.elastic.co/guide/en/logstash/current/output-plugins.html](https://www.elastic.co/guide/en/logstash/current/output-plugins.html)
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Basic logstash pipeline

<div align="left">A quick test to check your logstash installation:</div>

```
/opt/logstash/bin/logstash -e 'input { stdin {} } output { stdout {} }'
```

<aside class="notes"><p>Logstash will take 3 to 5 seconds to start and type some text in the terminal to get it as output (stdin -&gt; stdout)</p>
</aside></script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### A quick primer on Grok filters
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- The syntax for a grok pattern is `%{SYNTAX:SEMANTIC}`
- `SYNTAX` is the name of the pattern that will match your text.
    - For example: `1337` will be matched by the `NUMBER` pattern, `192.168.123.12` will be matched by the `IP` pattern.
- SEMANTIC is the identifier you give to the piece of text being matched.
E.g. `1337` could be the count and `192.168.123.12` could be a `client` making a request

```
%{NUMBER:count}
%{IP:client}
```

<aside class="notes"><p>Grok is based on pre created regex patterns</p>
</aside></script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Example grok

<div align="left">For the following log event:</div>

```
55.3.244.1 GET /index.html 15824 0.043
```

<div align="left">This would be the matching grok:</div>

```
%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}
```

</script>
        </section>
        <section data-markdown>
          <script type="text/template">
#### Consider the following Apache Log Event

```
123.249.19.22 - - [01/Feb/2015:14:12:13 +0000] "GET /manager/html HTTP/1.1" 404 448 "-" "Mozilla/3.0 (compatible; Indy Library)
```

</script>
        </section>
        <section data-markdown>
          <script type="text/template">
Using a regular expression!!

![Apache RegEx](images/apacheregex.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
Using Grok filter patterns :)

```
%{IPV4} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?)" %{NUMBER:response} (?:%{NUMBER:bytes}|-)
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
Things can get even more simpler using an *inbuilt* grok :) :)

```
%{COMBINEDAPACHELOG}
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Available Logstash Grok Patterns
- [https://grokdebug.herokuapp.com/patterns](https://grokdebug.herokuapp.com/patterns)
- [http://grokconstructor.appspot.com/](http://grokconstructor.appspot.com/)
- [https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns](https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns)
- [https://github.com/clay584/logstash_configs](https://github.com/clay584/logstash_configs)
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Create a logstash configuration to receive filebeat logs from infra VM

- Create the input file to receive logs from filebeat `vi /etc/logstash/conf.d/02-beats.conf` in the `monitor` machine

```
input {
  beats {
    port => 5044
    ssl => true
    ssl_certificate => "/etc/pki/tls/certs/logstash-forwarder.crt"
    ssl_key => "/etc/pki/tls/private/logstash-forwarder.key"
  }
}
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- Create the ssh logs filter file  `vi /etc/logstash/conf.d/10-ssh-log.conf`

```
filter {
 if [type] == "sshlog" {
  grok {
#    add_tag => [ "valid" ]

    match => [
      "message", "%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\[%{POSINT}\])?: %{WORD:login} password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}",
      "message", "%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\[%{POSINT}\])?: message repeated 2 times: \[ %{WORD:login} password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}",
      "message", "%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\[%{POSINT}\])?: %{WORD:login} password for invalid user %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}",
      "message", "%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\[%{POSINT}\])?: %{WORD:login} %{WORD:auth_method} for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}"
    ]
  }

# If we wanted to get only valid SSH logs
#  if "valid" not in [tags] {
#    drop { }
#  }

#  mutate {
#    remove_tag => [ "valid" ]
#    lowercase => [ "login" ]
#  }

  date {
    match => [ "timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
    locale => en
  }

  geoip {
    source => "ip"
  }
 }
}
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- Create the web logs filter file `vi /etc/logstash/conf.d/11-web-log.conf`

```
filter {
 if [type] == "weblog" {
  grok {
    match => {
      "message" => '%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}'
    }
  }

  date {
    match => [ "timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
    locale => en
  }

  geoip {
    source => "clientip"
  }

  useragent {
    source => "agent"
    target => "useragent"
  }
 }
}
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- Create the output elasticsearch file `vi /etc/logstash/conf.d/20-elastic-output.conf`

```
output {
  elasticsearch {
    hosts => ["localhost:9200"]
    #sniffing => true
    manage_template => false
    index => "infra-%{+YYYY.MM.dd}"
  }
}
```
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Generate SSL Configuration for logstash to receive logs from infra VM

- Run the below commands in `monitor` machine

```
mkdir -p /etc/pki/tls/{certs,private}
```

- Add the system IP information by adding the below line `vi /etc/ssl/openssl.cnf +225`

```
subjectAltName = IP: 192.168.56.101
```

<aside class="notes"><p>(Monitor VM) ELK Server IP address</p>
</aside></script>
      </section>
      <section data-markdown>
        <script type="text/template">
```
cd /etc/pki/tls
openssl req -config /etc/ssl/openssl.cnf -x509 -days 365 -batch -nodes -newkey rsa:2048 -keyout private/logstash-forwarder.key -out certs/logstash-forwarder.crt
```
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Start the Logstash service

```
service logstash restart
```
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Kibana

> Kibana is an open source analytics and visualization platform designed to work with Elasticsearch. You use Kibana to search, view, and interact with data stored in Elasticsearch indices
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Also start the kibana service

```
service kibana restart
```
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Understanding the Kibana UI

**Index Selection**

![](images/k-1.png)

- Index for our workshop is `infra-*`
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
**Adding multiple indices**

![](images/k-2.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
**Discovery**

![](images/k-3.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
**Mapped log in JSON format**

![](images/k-4.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
**Visualize**

![](images/k-5.png)

- Choose a new visualization to create charts, graphs, etc.
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
**Selecting the search source**

![](images/k-6.png)

- We can select the search source to create visualization. It can be saved search or new search
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
**Creating a pie chart**

![](images/k-7.png)

- Now we can select the aggregation, for example `count` and we can also give custom label to display
- Then create buckets splitting slices. select the field which you want to create a pie chart and select the size of the field to display.
- Once selection is done, Click on the play button to apply the changes
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
**Dashboard**

![](images/k-9.png)

- Dashboards and visualizations can be imported and exported in JSON format
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
**Status**

![](images/k-10.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
**Import/Export**

![](images/k-11.png)

</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Let's configure the infra VM
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### SSH into the infra VM

```
ssh -l infra 192.168.56.100
sudo -i
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Create filebeat configuration to send logs to the monitor VM

- Edit the configuration file by `vi /etc/filebeat/filebeat.yml`
- Note: replace the IP of the monitor VM at hosts

```
filebeat:
  prospectors:
    -
      paths:
        - /var/log/auth.log
      #  - /var/log/syslog
      #  - /var/log/*.log

      document_type: sshlog

    -
      paths:
        - /var/log/apache2/access.log

      document_type: weblog

  registry_file: /var/lib/filebeat/registry

output:
  logstash:
    hosts: ["192.168.56.101:5044"]
    bulk_max_size: 1024

    tls:
      certificate_authorities: ["/etc/pki/tls/certs/logstash-forwarder.crt"]

shipper:

logging:
  files:
    rotateeverybytes: 10485760 # = 10MB
```

Note: `hosts` IP address is (monitor VM) ELK server IP address
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Copy the `logstash-forwarder.crt` to infra VM from monitor VM

- scp the certificate from monitor VM to infra VM

```
mkdir -p /etc/pki/tls/certs/

cd /etc/pki/tls/certs/

scp monitor@192.168.56.101:/etc/pki/tls/certs/logstash-forwarder.crt /etc/pki/tls/certs/logstash-forwarder.crt
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Restart the filebeat service


```
service filebeat restart
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Generate the SSH attack traffic by running bruteforce script

- In the host machine run the below script

```
python SSHBruteForce.py -i 192.168.56.100 -p 22 -d True -U ./usernames.txt -P ./passwords.txt
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Generate the WEB attack traffic by running nikto scan

- In the host machine run the below command

```
Windows: nikto.bat -config nikto.conf -host http://192.168.56.100
Linux/Mac: perl nikto.pl -config nikto.conf -host http://192.168.56.100
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">

### Login to the elasticsearch head plugin to see the logs

- Navigate to the below URL to see that logs coming from infra VM

```
http://192.168.56.101:8080/_plugin/head
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
![Head Plugin with logs](images/head-data.bmp)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
![Head Plugin logs browser](images/head-data-1.bmp)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Opening Index in Kibana

- Use the username `elkadmin` and password `Null@123`

![Kibana Index](images/kibana-auth.bmp)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Create the filebeat index

- We have use `infra-*` as index name

![Kibana Index Creation](images/kibana-creation-1.bmp)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
![Kibana Discover](images/kibana-creation-2.bmp)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
![Kibana DSL query](images/kibana-creation-3.bmp)

</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### What is alerting for ELK stack?

> We can set up a notification system to let users/admins know that a pattern match has occurred.
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### How is this achieved?

- Logstash output plugin alerting via (Email, Pager duty, JIRA, etc.)
- Elasticsearch commercial product - Watcher
- An open source alerting for elasticsearch by Yelp called `elastalert`
- Custom scripts
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Introducing elastalert

An opensource project by Yelp for alerting on Elasticsearch

> ElastAlert is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch.
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Creating alerting for the attacks

- The elastalert is installed in `/opt/elastalert`
- Run the below commands to create elastalert index

```
elastalert-create-index
```

![elastalert creation](images/elastalert-creation.bmp)
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Creating rules for attacks
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Web 404 error logs attack rule

- `vi /opt/elastalert/web-404-error-slack.yml`

```
es_host: localhost
es_port: 9200
name: "web 404 error log alert"
type: frequency
index: infra-*
num_events: 20
timeframe:
  hours: 24

# For more info: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl.html

filter:
- query:
    query_string:
      query: "response: 404"

alert:
  - slack
  - email

slack:
slack_webhook_url: "https://hooks.slack.com/services/xxxxx"

email:
  - "root"

realert:
  minutes: 0
```

<aside class="notes"><p>Replace the Slack webhook</p>
</aside></script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Create slack webhook

![Slack login](images/slack/1-slack-login.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
![custom integration](images/slack/2-custom-integrations.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
![Add web hook](images/slack/3-add-web-hook.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
![add slack channel](images/slack/4-add-channel.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
![incoming web hook](images/slack/5-incoming-webhoook.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
![save the config](images/slack/6-save.png)
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Configuring the elastalert

- Replace the configuration file content `vi /opt/elastalert/config.yaml`

```
rules_folder: example_rules
run_every:
  minutes: 1
buffer_time:
  minutes: 15
es_host: localhost
es_port: 9200
writeback_index: elastalert_status
alert_time_limit:
  days: 2
```
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Running alerting on indices

```
elastalert --verbose --config /opt/elastalert/config.yaml --start 2016-10-10 --rule /opt/elastalert/web-404-error-slack.yml
```
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Check the slack channel

![Slack Alert](images/slack-alert.bmp)
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Check Email as well (Base64 Encoded)

![Email Alert](images/email-alert.bmp)

</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Backing up Elasticsearch
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### What is Curator ?

> Elasticsearch Curator helps you curate or manage your indices.
> Supports a variety of actions from delete a snapshot to shard allocation routing.

<br />

> This is an additional software bundle that you install on top of Elasticsearch.
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- Run the below command in monitor VM to setup the backups path

```
curl -XPUT 'http://localhost:9200/_snapshot/backup' -d '{
"type": "fs",
"settings": {
"location": "/var/backups/elasticsearch/",
"compress": true
}
}'
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### To snapshot an index called `elastalert_status` into a repository `backup`

```
curator snapshot --name=elastalert_logs_snapshot --repository backup indices --prefix elastalert_status
```

### To see all snapshots in the `backup` repository

```
curator show snapshots --repository backup
```

### To restore a snapshot from curator

```
curl -XPOST 'http://localhost:9200/_snapshot/backup/elastalert_logs_snapshot/_restore'
```
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Restore sample logs to create some advanced dashboards

```
tar -xvf /srv/filebeat.tar.gz -C /var/backups/elasticsearch/

curator show snapshots --repository backup

curl -XPOST 'http://localhost:9200/_snapshot/backup/filebeat_logs_snapshot/_restore'
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Look at head plugin to see the restored sample logs

- Navigate to http://192.168.56.101:8080/_plugin/head

![Filebeat sample logs](images/filebeat-sample-logs.bmp)
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">

## Dashboards for Attack Patterns

<aside class="notes"><p>We have already imported logs to Elasticsearch using curator. <br />
Now create some advanced dashboards for attack patterns using Kibana</p>
</aside></script>
        </section>
        <section data-markdown>
          <script type="text/template">
![Create index](images/k-1.png)
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Refresh the Fielbeat index field list

![Kibana index fields refresh](images/kibana-import-1.bmp)
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
- Select the JSON file in your USB drive `/dashboards/all-kibana.json`

![Kibana index import](images/kibana-import-2.bmp)
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Pre-created dashboards

- Access the pre-created dashboards using

![dashboards location](images/kibana-dashboards.bmp)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Web Attack Dashboard

![Web Attack Dashboard](images/kibana-web-dashboard.bmp)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### SSH Attack Dashboard

![SSH Attack Dashboard](images/kibana-ssh-dashboard.bmp)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Combined Attack Dashboard

![Attack Dashboard](images/kibana-attack-dashboard.bmp)
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Defence (ELK + AWS Lambda) - DEMO

<iframe frameborder=0 width=95% height=586 marginheight=0 marginwidth=0 scrolling=no src="https://youtube.com/embed/3_HIlDm3GtM?autoplay=0&controls=0&showinfo=0&autohide=1"></iframe>
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Slides

https://speakerdeck.com/madhuakula/all-day-devops-automated-infrastructure-security-monitoring-and-defence-elk-plus-aws-lambda
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
## Ansible Playbook for the entire setup
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## What is Ansible?

>  Simple IT automation engine that automates cloud provisioning, configuration management, application deployment, intra-service orchestration, and many other IT needs.
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## What is an Ansible playbook?

> playbooks are Ansible's configuration, deployment, and orchestration language (written in YAML). They can describe a policy that you want your remote systems to enforce, or a set of steps in a general IT process.
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
## A playbook for our setup

> We created a plug and play ansible playbook, which helps to build and reproduce the same environment within minutes by provisioning in any cloud and stack.
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
# Pentesting the infrastructure
(what does an attacker see?)
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
## The Why?
A penetration testing exercise was undertaken to check if there was a possibility of gaining unauthorized access to the setup
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## Black Box Penetration Testing
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Port scanning
![Port scan to discover services](images/pentest/nmapscan.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Service enumeration
![Service enumeration scan](images/pentest/serviceenum.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### HTTP basic Auth on ports 80 and 8080
![HTTP basic Auth](images/pentest/httpAuth.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Attempted brute force
Multiple dictionaries were tried against the HTTP Basic Auth
![Hydra HTTP Basic Brute Force](images/pentest/hydrapasscrack.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Attempted brute force
Multiple dictionaries were tried against SSH as well
![Hydra SSH Brute Force](images/pentest/hydrasshpasscrack.png)
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## Grey Box Penetration Testing
App credentials were provided
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Verbose Errors
![Verbose Kibana stack traces](images/pentest/kibana_verbose_error.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Credential Leakage through MiTM

![Request Response having the Basic Auth header](images/pentest/BasicAuthOverHTTP.png)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
![MITM decoded password](images/pentest/BasicAuthOverHTTP-2.png)
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
## ELK - Exercise with network logs
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
## Stack Flow

#### CSV firewall log(network.log) -> logstash(network.conf) -> elasticsearch -> kibana(kibana-network.json)
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- `network.log` file is at `/srv/network.log`
- `network.conf` file is in your host system configurations `network.conf` for logstash configuration
- Build your own dashboard using the log data
- We created one. If you want to use, import the `/dashboards/kibana-network.json`
- Happy ELK!
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Network Security Monitoring

![Network Dashboard](images/network-dashboard.png)
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
### Password Dump Analysis

![Password Dump Dashboard](images/pwd-analysis.png)

> ELK can also be used to do other amazing things like correlate password dumps for example and give a visual statistical model of the data contained in the dump.
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Alternatives to the components

Given the large environment of available tools to standardise and correlate logs, we can often create a setup using other components that talk to each other.

- [fluentd](http://www.fluentd.org/) is an alternative for logstash
- [Grafana](http://grafana.org/) is an alternative for Kibana
- [Druid](http://druid.io/) is a high-performance, column-oriented, distributed data store as an alternative Elasticsearch
- [Riemann](http://riemann.io/) aggregates events from servers and applications with a powerful stream processing language.
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
### Full stack alternatives

- [TICK Stack](https://www.influxdata.com/get-started/what-is-the-tick-stack/)
- [Graylog](https://www.graylog.org/)

many more.
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### General Best Practice
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Elasticsearch Best Practice
- set `$ES_HEAP_SIZE` env var to 1/2 of RAM (but < 32GB)
- Disable memory swapping by enabling `bootstrap.mlockall`
- Set user's file `ulimit` to `unlimited` (Need reboot to check)
    - You can check with an API call as well `/_nodes/process`
- Use the default configuration and make small changes as required
- Multicast is great, but when you are going to production make sure to use `Unicast` discovery mode
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- To eliminate the "Split-brain" problem use 3 lower resource master-eligible nodes in larger cluster environments (dedicated)
- You don't need beefy machines, simple machines are ok due to the distributed nature of elasticsearch
- Add lightweight client nodes (no data)
- Use Snapshot and Restore. This is very useful (but different from `replication`)
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Logstash Best Practice

- Watch out for Grok Filter data (GREEDYDATA) as they use a lot of resources especially CPU and Memory. Try to get as specific as possible
- Test your configuration with `-e` `input{...}... output{...}`
- Use `-b` flag to send bulk requests to elasticsearch
- Use `-w` flag to utilise multiple cores. This is especially useful for multi core and bulk processing
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- Use the generator input for benchmarking (https://github.com/matejzero/logstash-benchmark) and to understand performance and optimisation metrics
- If something goes wrong try `-- debug` for more detailed output (don't forgot to turn this off when you are done)
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Kibana Best Practice

- Tune Queries in elasticsearch for maximum performance
- Configuring number of threads in pool
- Save and Export dashboards as a JSON File for reuse
- Deploy a proxy so that you can do basic authentication and other load balancing services
- While Kibana is an exploration tool, make sure you watch out for over-eager users affecting performance
</script>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Production

- Access Control / Security
    - use nginx/apache to setup basic authentication
    - You can block `POST` /  `PUT` / `DELETE` operations
    - Disable Scripting (Version < 1.2)
    `script.disable_dynamic: true`
    - Disable destructive actions
    `action.destructive_requires_name: true`
    - Use aliases to allow users access to subsets of indices
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- VM vs Metal
    - VM's are convenient (Auto scaling, no management, etc)
    - Bare metal is generally more configurable and higher in performance
    - Metal can utilize SSD's
    - Cloud VM's can suffer from `noisy neighbors`
    - But you should start using what you're most familiar with!
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- Disks
    - Spinning disk's are cheaper per GB
    - SSDs have better IOPS
    - SSDs are cheaper wrt: IOPS
    - SSD manufacturing tolerance can vary (vendor based)
    - SAN / NAS can work, if IOPS are sufficient (throughput, iops, etc)
    - Don't necessarily need RAID, ES handles redundancy
        - But striping can help with performance
        - You can use shards and replicas in ES
</script>
        </section>
      </section>
      <section>
        <section data-markdown>
          <script type="text/template">
### Security

- Harden the base server with traditional security techniques
- Use SSH key for login
- Remove root login
- Use randomly generated passwords
```
openssl rand -base64 24
```
</script>
        </section>
        <section data-markdown>
          <script type="text/template">
- Enable the host firewall and allow only connections from specific IPs
- Use SSL certificates and enable HTTPS for Elasticsearch, Logstash & Kibana (E.g.: Lets encrypt)
- Use search guard for granular permissions and role based authentication for ELK (Shield is an alternative)
</script>
        </section>
      </section>
      <section data-markdown>
        <script type="text/template">
### Monitoring Services

- Enable service level monitoring for Elasticsearch, Logstash and Kibana
- Use monit (or) uptime robot for monitoring services (you can also use Icinga)
</script>
      </section>
      <section data-markdown>
        <script type="text/template">
## Thank You for attending

###@madhuakula


</script>
      </section>
    </div>
  </div>

  <script src="./js/reveal.js"></script>

  <script>
    function extend() {
      var target = {};
      for (var i = 0; i < arguments.length; i++) {
        var source = arguments[i];
        for (var key in source) {
          if (source.hasOwnProperty(key)) {
            target[key] = source[key];
          }
        }
      }
      return target;
    }

    // Optional libraries used to extend on reveal.js
    var deps = [
      { src: './plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
      { src: './plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
      { src: './plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } },
      { src: './plugin/zoom-js/zoom.js', async: true },
      { src: './plugin/notes/notes.js', async: true },
      { src: './plugin/math/math.js', async: true }
    ];

    // default options to init reveal.js
    var defaultOptions = {
      controls: true,
      progress: true,
      history: true,
      center: true,
      transition: 'default', // none/fade/slide/convex/concave/zoom
      dependencies: deps
    };

    // options from URL query string
    var queryOptions = Reveal.getQueryHash() || {};

    var options = extend(defaultOptions, {}, queryOptions);
  </script>


  <script>
    Reveal.initialize(options);
  </script>
  <script type="application/javascript">
    var doNotTrack = false;
    if (!doNotTrack) {
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) }; ga.l = +new Date;
      ga('create', 'UA-15752161-2', 'auto');

      ga('send', 'pageview');
    }
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
</body>

</html>